{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2495f58-8a15-414b-aeb4-7aac972af6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Using cached dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a47fbd-54a0-4303-9930-bec21ee4af13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b02256-6f46-482a-aca7-58f371706d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc8cb5c-6ba8-4070-b487-a3bcecffe5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Using cached numpy-2.2.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sahil\\github\\sp25_taxi\\sp25_taxi\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahil\\github\\sp25_taxi\\sp25_taxi\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached numpy-2.2.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.3 pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62dbc040-c179-4893-8bca-7c556d757394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download raw data from 2023 to 2024\n",
      "File already exists for 2023-01.\n",
      "Loading data for 2023-01...\n",
      "Error processing data for 2023-01: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-02.\n",
      "Loading data for 2023-02...\n",
      "Error processing data for 2023-02: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-03.\n",
      "Loading data for 2023-03...\n",
      "Error processing data for 2023-03: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-04.\n",
      "Loading data for 2023-04...\n",
      "Error processing data for 2023-04: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-05.\n",
      "Loading data for 2023-05...\n",
      "Error processing data for 2023-05: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-06.\n",
      "Loading data for 2023-06...\n",
      "Error processing data for 2023-06: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-07.\n",
      "Loading data for 2023-07...\n",
      "Error processing data for 2023-07: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-08.\n",
      "Loading data for 2023-08...\n",
      "Error processing data for 2023-08: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-09.\n",
      "Loading data for 2023-09...\n",
      "Error processing data for 2023-09: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-10.\n",
      "Loading data for 2023-10...\n",
      "Error processing data for 2023-10: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-11.\n",
      "Loading data for 2023-11...\n",
      "Error processing data for 2023-11: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      "File already exists for 2023-12.\n",
      "Loading data for 2023-12...\n",
      "Error processing data for 2023-12: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No data could be loaded for the year 2023 and specified months: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m chunks = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(from_year, to_year+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     rides_one_year = \u001b[43mload_and_process_taxi_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     chunks.append(rides_one_year)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Concatenate all chunks at the end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\github\\sp25_taxi\\src\\data_utils.py:162\u001b[39m, in \u001b[36mload_and_process_taxi_data\u001b[39m\u001b[34m(year, months)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Combine all monthly data\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m monthly_rides:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo data could be loaded for the year \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and specified months: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonths\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCombining all monthly data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    167\u001b[39m combined_rides = pd.concat(monthly_rides, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mException\u001b[39m: No data could be loaded for the year 2023 and specified months: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from src.data_utils import load_and_process_taxi_data\n",
    "\n",
    "from_year = 2023\n",
    "# to_year = datetime.now().year\n",
    "to_year = 2024\n",
    "print(f\"Download raw data from {from_year} to {to_year}\")\n",
    "\n",
    "rides = pd.DataFrame()\n",
    "chunks = []\n",
    "for year in range(from_year, to_year+1):\n",
    "\n",
    "    rides_one_year = load_and_process_taxi_data(year)\n",
    "\n",
    "    chunks.append(rides_one_year)\n",
    "    \n",
    "\n",
    "# Concatenate all chunks at the end\n",
    "rides = pd.concat(chunks, ignore_index=True)\n",
    "print(\"Data loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce5589a2-9316-4dac-ba2e-e9a8a4789241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f47e90-0fcb-4b7a-80d0-a68c979f9ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5259f4bd-65ce-43dc-b487-09ee12d964bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pickup_datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transform_raw_data_into_ts_data\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ts_data = \u001b[43mtransform_raw_data_into_ts_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\github\\sp25_taxi\\src\\data_utils.py:223\u001b[39m, in \u001b[36mtransform_raw_data_into_ts_data\u001b[39m\u001b[34m(rides)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03mTransform raw ride data into time series format.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m \u001b[33;03m    pd.DataFrame: Time series data with filled gaps\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Floor datetime to hour efficiently\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m rides[\u001b[33m\"\u001b[39m\u001b[33mpickup_hour\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mrides\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpickup_datetime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.dt.floor(\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Aggregate and fill gaps\u001b[39;00m\n\u001b[32m    226\u001b[39m agg_rides = (\n\u001b[32m    227\u001b[39m     rides.groupby([\u001b[33m\"\u001b[39m\u001b[33mpickup_hour\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpickup_location_id\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    228\u001b[39m     .size()\n\u001b[32m    229\u001b[39m     .reset_index(name=\u001b[33m\"\u001b[39m\u001b[33mrides\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    230\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\github\\sp25_taxi\\sp25_taxi\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\github\\sp25_taxi\\sp25_taxi\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'pickup_datetime'"
     ]
    }
   ],
   "source": [
    "from src.data_utils import transform_raw_data_into_ts_data\n",
    "\n",
    "ts_data = transform_raw_data_into_ts_data(rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59252b5f-17fc-4207-a1c4-6bebd49233ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4561440, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92402d0c-dacd-4039-ba40-caf7eeafe8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4561440 entries, 0 to 4561439\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   pickup_hour         datetime64[ns]\n",
      " 1   pickup_location_id  int16         \n",
      " 2   rides               int16         \n",
      "dtypes: datetime64[ns](1), int16(2)\n",
      "memory usage: 52.2 MB\n"
     ]
    }
   ],
   "source": [
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3804e864-fcde-41f4-bf92-5c0e8e90aa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561435</th>\n",
       "      <td>2024-12-31 19:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561436</th>\n",
       "      <td>2024-12-31 20:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561437</th>\n",
       "      <td>2024-12-31 21:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561438</th>\n",
       "      <td>2024-12-31 22:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561439</th>\n",
       "      <td>2024-12-31 23:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4561440 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                pickup_hour  pickup_location_id  rides\n",
       "0       2023-01-01 00:00:00                   2      0\n",
       "1       2023-01-01 01:00:00                   2      0\n",
       "2       2023-01-01 02:00:00                   2      0\n",
       "3       2023-01-01 03:00:00                   2      0\n",
       "4       2023-01-01 04:00:00                   2      0\n",
       "...                     ...                 ...    ...\n",
       "4561435 2024-12-31 19:00:00                 263    148\n",
       "4561436 2024-12-31 20:00:00                 263    231\n",
       "4561437 2024-12-31 21:00:00                 263    247\n",
       "4561438 2024-12-31 22:00:00                 263    193\n",
       "4561439 2024-12-31 23:00:00                 263     92\n",
       "\n",
       "[4561440 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "451d7311-7a98-4f02-9343-bc7f8ced427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in c:\\users\\sahil\\anaconda3\\lib\\site-packages (2.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "980aaa20-c0aa-447a-abac-e012b97c83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hopsworks\n",
      "  Using cached hopsworks-4.1.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyhumps==1.6.1 (from hopsworks)\n",
      "  Using cached pyhumps-1.6.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from hopsworks) (2.32.2)\n",
      "Collecting furl (from hopsworks)\n",
      "  Using cached furl-2.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: boto3 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from hopsworks) (1.35.85)\n",
      "Collecting pandas<2.2.0 (from hopsworks)\n",
      "  Using cached pandas-2.1.4-cp312-cp312-win_amd64.whl.metadata (18 kB)\n",
      "Collecting pyjks (from hopsworks)\n",
      "  Using cached pyjks-20.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mock (from hopsworks)\n",
      "  Using cached mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting avro==1.11.3 (from hopsworks)\n",
      "  Using cached avro-1.11.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from hopsworks) (2.0.30)\n",
      "Collecting PyMySQL[rsa] (from hopsworks)\n",
      "  Using cached PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting tzlocal (from hopsworks)\n",
      "  Using cached tzlocal-5.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from hopsworks) (2024.3.1)\n",
      "Collecting retrying (from hopsworks)\n",
      "  Using cached retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting hopsworks_aiomysql==0.2.1 (from hopsworks_aiomysql[sa]==0.2.1->hopsworks)\n",
      "  Using cached hopsworks_aiomysql-0.2.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting opensearch-py<=2.4.2,>=1.1.0 (from hopsworks)\n",
      "  Using cached opensearch_py-2.4.2-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from hopsworks) (4.66.4)\n",
      "Collecting grpcio<2.0.0,>=1.49.1 (from hopsworks)\n",
      "  Using cached grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting protobuf<5.0.0,>=4.25.4 (from hopsworks)\n",
      "  Using cached protobuf-4.25.6-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: packaging in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from hopsworks) (23.2)\n",
      "Collecting sqlalchemy (from hopsworks)\n",
      "  Using cached SQLAlchemy-2.0.29-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from opensearch-py<=2.4.2,>=1.1.0->hopsworks) (2024.7.4)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from pandas<2.2.0->hopsworks) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from pandas<2.2.0->hopsworks) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from pandas<2.2.0->hopsworks) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from requests->hopsworks) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from requests->hopsworks) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from sqlalchemy->hopsworks) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from sqlalchemy->hopsworks) (3.0.1)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.85 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from boto3->hopsworks) (1.35.85)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from boto3->hopsworks) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from boto3->hopsworks) (0.10.4)\n",
      "Collecting orderedmultidict>=1.0.1 (from furl->hopsworks)\n",
      "  Using cached orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting javaobj-py3 (from pyjks->hopsworks)\n",
      "  Using cached javaobj_py3-0.4.4-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pyasn1>=0.3.5 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from pyjks->hopsworks) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from pyjks->hopsworks) (0.2.8)\n",
      "Collecting pycryptodomex (from pyjks->hopsworks)\n",
      "  Using cached pycryptodomex-3.21.0-cp36-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting twofish (from pyjks->hopsworks)\n",
      "  Using cached twofish-0.3.0.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cryptography in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from PyMySQL[rsa]->hopsworks) (42.0.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tqdm->hopsworks) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from cryptography->PyMySQL[rsa]->hopsworks) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography->PyMySQL[rsa]->hopsworks) (2.21)\n",
      "Using cached hopsworks-4.1.8-py3-none-any.whl (646 kB)\n",
      "Using cached hopsworks_aiomysql-0.2.1-py3-none-any.whl (44 kB)\n",
      "Using cached pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
      "Using cached grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached opensearch_py-2.4.2-py2.py3-none-any.whl (258 kB)\n",
      "Using cached pandas-2.1.4-cp312-cp312-win_amd64.whl (10.5 MB)\n",
      "Using cached protobuf-4.25.6-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached SQLAlchemy-2.0.29-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Using cached furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Using cached mock-5.2.0-py3-none-any.whl (31 kB)\n",
      "Using cached pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
      "Using cached retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Using cached tzlocal-5.3-py3-none-any.whl (17 kB)\n",
      "Using cached orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "Using cached javaobj_py3-0.4.4-py2.py3-none-any.whl (57 kB)\n",
      "Using cached pycryptodomex-3.21.0-cp36-abi3-win_amd64.whl (1.8 MB)\n",
      "Building wheels for collected packages: twofish\n",
      "  Building wheel for twofish (setup.py): started\n",
      "  Building wheel for twofish (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for twofish\n",
      "Failed to build twofish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [1 lines of output]\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for twofish\n",
      "ERROR: Could not build wheels for twofish, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "623726bf-54f4-4dce-8342-649cac1f396d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hopsworks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhopsworks\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hopsworks'"
     ]
    }
   ],
   "source": [
    "import hopsworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8949a20a-f5da-45ff-990f-d789d234cdf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hopsworks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m project_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOPSWORKS_PROJECT_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# pip install confluent-kafka\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize connection to Hopsworks  \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m project \u001b[38;5;241m=\u001b[39m \u001b[43mhopsworks\u001b[49m\u001b[38;5;241m.\u001b[39mlogin(  \n\u001b[0;32m      7\u001b[0m     api_key_value\u001b[38;5;241m=\u001b[39mapi_key,  \n\u001b[0;32m      8\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject_name  \n\u001b[0;32m      9\u001b[0m )  \n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully connected to Hopsworks project: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hopsworks' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "api_key = os.getenv('HOPSWORKS_API_KEY')  \n",
    "project_name = os.getenv('HOPSWORKS_PROJECT_NAME')  \n",
    "\n",
    "# pip install confluent-kafka\n",
    "# Initialize connection to Hopsworks  \n",
    "project = hopsworks.login(  \n",
    "    api_key_value=api_key,  \n",
    "    project=project_name  \n",
    ")  \n",
    "print(f\"Successfully connected to Hopsworks project: {project_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "348b1a4b-6141-4078-bc9f-e9e3a750a0f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m feature_store \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[38;5;241m.\u001b[39mget_feature_store()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'project' is not defined"
     ]
    }
   ],
   "source": [
    "feature_store = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54682e-b7d3-49fc-86bd-65b1b06206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GROUP_NAME = \"time_series_hourly_feature_group\"\n",
    "FEATURE_GROUP_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f292eed-c4a1-4d3a-8ee9-daa7bb1fea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = feature_store.get_feature_group(\n",
    "    name=FEATURE_GROUP_NAME,\n",
    "    version=FEATURE_GROUP_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec573dd2-3125-4d2f-93a9-975bedfe739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.insert(ts_data, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff98cc-31f4-47b4-a43c-fe731760af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory_mb = rides.memory_usage(deep=True).sum() / (1024 * 1024)  \n",
    "print(f\"DataFrame size: {df_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be636b4b-4bd5-469d-8cc7-b09e1de29ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
